{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('all')\n",
    "import re\n",
    "import numpy as np\n",
    "import collections\n",
    "import sklearn as sk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Recommended or Not Recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unnamed holophone gluten fundamentals dividen ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thoughtthrough chargeor gift prepurchased upat...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reopening sequences boba ультранизкая leaked f...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>domingo skimming esse rollercoaster etkili ski...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chooseyourownadventure solitary een stairs yee...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16455</th>\n",
       "      <td>bigbudget ух railways definition wooooaaa unin...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16456</th>\n",
       "      <td>horror freakishly kurguluyor chunky cobwebs bc...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16457</th>\n",
       "      <td>sinful abstracted attentionon sunlight brecomm...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16458</th>\n",
       "      <td>charactercar daemons lt cruising ахуенно slews...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16459</th>\n",
       "      <td>telling amzing hslow cream turds upholds bins ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16460 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  \\\n",
       "0      unnamed holophone gluten fundamentals dividen ...   \n",
       "1      thoughtthrough chargeor gift prepurchased upat...   \n",
       "2      reopening sequences boba ультранизкая leaked f...   \n",
       "3      domingo skimming esse rollercoaster etkili ski...   \n",
       "4      chooseyourownadventure solitary een stairs yee...   \n",
       "...                                                  ...   \n",
       "16455  bigbudget ух railways definition wooooaaa unin...   \n",
       "16456  horror freakishly kurguluyor chunky cobwebs bc...   \n",
       "16457  sinful abstracted attentionon sunlight brecomm...   \n",
       "16458  charactercar daemons lt cruising ахуенно slews...   \n",
       "16459  telling amzing hslow cream turds upholds bins ...   \n",
       "\n",
       "       Recommended or Not Recommended  \n",
       "0                                True  \n",
       "1                                True  \n",
       "2                                True  \n",
       "3                                True  \n",
       "4                                True  \n",
       "...                               ...  \n",
       "16455                           False  \n",
       "16456                           False  \n",
       "16457                           False  \n",
       "16458                           False  \n",
       "16459                           False  \n",
       "\n",
       "[16460 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../B_Data_Cleaning/synthetic_reviews_all.csv\")\n",
    "df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "df[\"Review\"] = df[\"Review\"].astype(\"str\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    16460\n",
       "Name: Review, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the length of all the reviews\n",
    "(df[\"Review\"]=='').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Recommended or Not Recommended</th>\n",
       "      <th>set_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unnamed holophone gluten fundamentals dividen ...</td>\n",
       "      <td>True</td>\n",
       "      <td>{atlast, lingo, noti, beach, itinyi, disgraced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thoughtthrough chargeor gift prepurchased upat...</td>\n",
       "      <td>True</td>\n",
       "      <td>{tennis, gunfight, goodness, coats, customize,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reopening sequences boba ультранизкая leaked f...</td>\n",
       "      <td>True</td>\n",
       "      <td>{mightve, fluro, hyped, jogar, satupid, realiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>domingo skimming esse rollercoaster etkili ski...</td>\n",
       "      <td>True</td>\n",
       "      <td>{libertarianright, concluídos, chart, acaba, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chooseyourownadventure solitary een stairs yee...</td>\n",
       "      <td>True</td>\n",
       "      <td>{ehi, hulahand, justly, verhicles, buggery, em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16455</th>\n",
       "      <td>bigbudget ух railways definition wooooaaa unin...</td>\n",
       "      <td>False</td>\n",
       "      <td>{fabricar, opposite, завязка, reconstructions,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16456</th>\n",
       "      <td>horror freakishly kurguluyor chunky cobwebs bc...</td>\n",
       "      <td>False</td>\n",
       "      <td>{massively, eventgig, recurring, berserker, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16457</th>\n",
       "      <td>sinful abstracted attentionon sunlight brecomm...</td>\n",
       "      <td>False</td>\n",
       "      <td>{exploitation, wizzy, أسلوب, modifications, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16458</th>\n",
       "      <td>charactercar daemons lt cruising ахуенно slews...</td>\n",
       "      <td>False</td>\n",
       "      <td>{lt, hmechanicsh, blergh, играм, rdrgta, tomor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16459</th>\n",
       "      <td>telling amzing hslow cream turds upholds bins ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{wasza, مفقعه, ownh, smattering, customerorien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16460 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  \\\n",
       "0      unnamed holophone gluten fundamentals dividen ...   \n",
       "1      thoughtthrough chargeor gift prepurchased upat...   \n",
       "2      reopening sequences boba ультранизкая leaked f...   \n",
       "3      domingo skimming esse rollercoaster etkili ski...   \n",
       "4      chooseyourownadventure solitary een stairs yee...   \n",
       "...                                                  ...   \n",
       "16455  bigbudget ух railways definition wooooaaa unin...   \n",
       "16456  horror freakishly kurguluyor chunky cobwebs bc...   \n",
       "16457  sinful abstracted attentionon sunlight brecomm...   \n",
       "16458  charactercar daemons lt cruising ахуенно slews...   \n",
       "16459  telling amzing hslow cream turds upholds bins ...   \n",
       "\n",
       "       Recommended or Not Recommended  \\\n",
       "0                                True   \n",
       "1                                True   \n",
       "2                                True   \n",
       "3                                True   \n",
       "4                                True   \n",
       "...                               ...   \n",
       "16455                           False   \n",
       "16456                           False   \n",
       "16457                           False   \n",
       "16458                           False   \n",
       "16459                           False   \n",
       "\n",
       "                                              set_column  \n",
       "0      {atlast, lingo, noti, beach, itinyi, disgraced...  \n",
       "1      {tennis, gunfight, goodness, coats, customize,...  \n",
       "2      {mightve, fluro, hyped, jogar, satupid, realiz...  \n",
       "3      {libertarianright, concluídos, chart, acaba, p...  \n",
       "4      {ehi, hulahand, justly, verhicles, buggery, em...  \n",
       "...                                                  ...  \n",
       "16455  {fabricar, opposite, завязка, reconstructions,...  \n",
       "16456  {massively, eventgig, recurring, berserker, ma...  \n",
       "16457  {exploitation, wizzy, أسلوب, modifications, ma...  \n",
       "16458  {lt, hmechanicsh, blergh, играм, rdrgta, tomor...  \n",
       "16459  {wasza, مفقعه, ownh, smattering, customerorien...  \n",
       "\n",
       "[16460 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a set_column to count the number of words in each review\n",
    "df[\"set_column\"] = df[\"Review\"].apply(lambda x: set(x.split()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "train, test = sk.model_selection.train_test_split(df, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing dataset into recommended and not recommended\n",
    "df_recom = train.loc[train[\"Recommended or Not Recommended\"] == True,:]\n",
    "df_recom = df_recom.reset_index(drop=True)\n",
    "\n",
    "df_not_recom = train.loc[train[\"Recommended or Not Recommended\"] == False,:]\n",
    "df_not_recom = df_not_recom.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to make the set dictionary\n",
    "# count how many times each word appears in the reviews\n",
    "bow_recom_temp = collections.Counter([y for x in df_recom.set_column for y in x])\n",
    "bow_not_recom_temp = collections.Counter([y for x in df_not_recom.set_column for y in x])\n",
    "\n",
    "bow_recom_set = dict(bow_recom_temp)\n",
    "bow_not_recom_set = dict(bow_not_recom_temp)\n",
    "\n",
    "for key in bow_recom_set:\n",
    "    if key not in bow_not_recom_set:\n",
    "        bow_not_recom_set[key] = 0\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for key in bow_not_recom_set:\n",
    "    if key not in bow_recom_set:\n",
    "        bow_recom_set[key] = 0\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for key in bow_recom_set:\n",
    "    bow_recom_set[key] += 1\n",
    "\n",
    "for key in bow_not_recom_set:\n",
    "    bow_not_recom_set[key] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to make the list dictionary\n",
    "# count how many times each word appears in the reviews\n",
    "bow_recom = collections.Counter([y for x in df_recom.Review for y in x.split()])\n",
    "bow_not_recom = collections.Counter([y for x in df_not_recom.Review for y in x.split()])\n",
    "\n",
    "bow_recom_dict = dict(bow_recom)\n",
    "bow_not_recom_dict = dict(bow_not_recom)\n",
    "\n",
    "for key in bow_recom_dict:\n",
    "    if key not in bow_not_recom_dict:\n",
    "        bow_not_recom_dict[key] = 0\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for key in bow_not_recom_dict:\n",
    "    if key not in bow_recom_dict:\n",
    "        bow_recom_dict[key] = 0\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for key in bow_recom_dict:\n",
    "    bow_recom_dict[key] += 1\n",
    "\n",
    "for key in bow_not_recom_dict:\n",
    "    bow_not_recom_dict[key] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_recom_sum_vals = sum(bow_recom_dict.values())\n",
    "bow_not_recom_sum_vals = sum(bow_not_recom_dict.values())\n",
    "\n",
    "for i in bow_recom_dict:\n",
    "\n",
    "    bow_recom_dict[i] /= bow_recom_sum_vals\n",
    "\n",
    "for i in bow_not_recom_dict:\n",
    "\n",
    "    bow_not_recom_dict[i] /= bow_not_recom_sum_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(bow_recom_dict) == len(bow_not_recom_dict)\n",
    "assert len(bow_recom_set) == len(bow_not_recom_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns=[\"set_column\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sentiment_all(element,test1=False, test2=False):\n",
    "\n",
    "    \"\"\"This function takes a review and returns the label for that review\"\"\"\n",
    "\n",
    "    full_shape = df.shape[0]\n",
    "    positive_review_probabiliy = len(df_recom)/full_shape\n",
    "    negative_review_probabiliy = len(df_not_recom)/full_shape\n",
    "    \n",
    "    df_choices_positive = [positive_review_probabiliy, bow_recom_set, bow_recom_dict, df_recom]\n",
    "    df_choices_negative = [negative_review_probabiliy ,bow_not_recom_set, bow_not_recom_dict, df_not_recom]\n",
    "\n",
    "    def classifier(element, your_class = 'positive'):\n",
    "        flag = {}\n",
    "        flag_count = 0\n",
    "        if your_class == 'positive':\n",
    "            df_choices = df_choices_positive\n",
    "        else:\n",
    "            df_choices = df_choices_negative\n",
    "        prob_of_class = df_choices[0]/full_shape\n",
    "        score = 1 * prob_of_class\n",
    "        score = float(format(score, '.12f'))\n",
    "        # score = 0.1\n",
    "        for i in element.split():\n",
    "            if i not in df_choices[2].keys():\n",
    "                pass\n",
    "            else:\n",
    "                prob_word_given_class = (df_choices[2])[i]\n",
    "                prob_word_given_class = float(format(prob_word_given_class, '.12f'))\n",
    "                # Almost the same value, given our spin on this application. \n",
    "                # Normally, this term frequency would be calculated differently across the positive and negative documents\n",
    "                # but we are only looking at the reviews as the documents themselves to determine a word's relevance in the positive\n",
    "                # or negative corpus. \n",
    "                tf = np.log(prob_word_given_class)\n",
    "                # tf = float(format(tf, '.12f'))\n",
    "                # tf = abs(np.log(prob_word_given_class))\n",
    "                # The IDF is the number of reviews / the number of reviews that contain the word in that given corpus\n",
    "                # idf = abs(np.log(df_choices[3].shape[0]/(df_choices[1])[i]))\n",
    "                idf = np.log(df_choices[3].shape[0]/(df_choices[1])[i])\n",
    "                # idf = float(format(idf, '.12f'))\n",
    "                score *= prob_word_given_class*tf*idf\n",
    "                # score *= prob_word_given_class*idf\n",
    "                # score *= prob_word_given_class*tf\n",
    "                # score *= prob_word_given_class\n",
    "\n",
    "                if score < 0:\n",
    "                    flag[flag_count] = (score, your_class)\n",
    "                    print(flag_count,i)\n",
    "                    flag_count += 1\n",
    "\n",
    "\n",
    "        return score\n",
    "    positive_score = classifier(element, 'positive')\n",
    "    negative_score = classifier(element, 'negative')\n",
    "    if positive_score > negative_score:\n",
    "        return True\n",
    "    elif positive_score == negative_score:\n",
    "        # Choosing an arbitrary value, because we assume that a review with one or few words of little substance\n",
    "        # is implied to be negative, as is usual with netizens. \n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sentiment_idf(element,test1=False, test2=False):\n",
    "\n",
    "    \"\"\"This function takes a review and returns the label for that review\"\"\"\n",
    "\n",
    "    full_shape = df.shape[0]\n",
    "    positive_review_probabiliy = len(df_recom)/full_shape\n",
    "    negative_review_probabiliy = len(df_not_recom)/full_shape\n",
    "    \n",
    "    df_choices_positive = [positive_review_probabiliy, bow_recom_set, bow_recom_dict, df_recom]\n",
    "    df_choices_negative = [negative_review_probabiliy ,bow_not_recom_set, bow_not_recom_dict, df_not_recom]\n",
    "\n",
    "    def classifier(element, your_class = 'positive'):\n",
    "        flag = {}\n",
    "        flag_count = 0\n",
    "        if your_class == 'positive':\n",
    "            df_choices = df_choices_positive\n",
    "        else:\n",
    "            df_choices = df_choices_negative\n",
    "        prob_of_class = df_choices[0]/full_shape\n",
    "        score = 1 * prob_of_class\n",
    "        score = float(format(score, '.12f'))\n",
    "        # score = 0.1\n",
    "        for i in element.split():\n",
    "            if i not in df_choices[2].keys():\n",
    "                pass\n",
    "            else:\n",
    "                prob_word_given_class = (df_choices[2])[i]\n",
    "                prob_word_given_class = float(format(prob_word_given_class, '.12f'))\n",
    "                # Almost the same value, given our spin on this application. \n",
    "                # Normally, this term frequency would be calculated differently across the positive and negative documents\n",
    "                # but we are only looking at the reviews as the documents themselves to determine a word's relevance in the positive\n",
    "                # or negative corpus. \n",
    "                tf = np.log(prob_word_given_class)\n",
    "                # tf = float(format(tf, '.12f'))\n",
    "                # tf = abs(np.log(prob_word_given_class))\n",
    "                # The IDF is the number of reviews / the number of reviews that contain the word in that given corpus\n",
    "                # idf = abs(np.log(df_choices[3].shape[0]/(df_choices[1])[i]))\n",
    "                idf = np.log(df_choices[3].shape[0]/(df_choices[1])[i])\n",
    "                # idf = float(format(idf, '.12f'))\n",
    "                # score *= prob_word_given_class*tf*idf\n",
    "                score *= prob_word_given_class*idf\n",
    "                # score *= prob_word_given_class*tf\n",
    "                # score *= prob_word_given_class\n",
    "\n",
    "                if score < 0:\n",
    "                    flag[flag_count] = (score, your_class)\n",
    "                    print(flag_count,i)\n",
    "                    flag_count += 1\n",
    "\n",
    "\n",
    "        return score\n",
    "    positive_score = classifier(element, 'positive')\n",
    "    negative_score = classifier(element, 'negative')\n",
    "    if positive_score > negative_score:\n",
    "        return True\n",
    "    elif positive_score == negative_score:\n",
    "        # Choosing an arbitrary value, because we assume that a review with one or few words of little substance\n",
    "        # is implied to be negative, as is usual with netizens. \n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sentiment_tf(element,test1=False, test2=False):\n",
    "\n",
    "    \"\"\"This function takes a review and returns the label for that review\"\"\"\n",
    "\n",
    "    full_shape = df.shape[0]\n",
    "    positive_review_probabiliy = len(df_recom)/full_shape\n",
    "    negative_review_probabiliy = len(df_not_recom)/full_shape\n",
    "    \n",
    "    df_choices_positive = [positive_review_probabiliy, bow_recom_set, bow_recom_dict, df_recom]\n",
    "    df_choices_negative = [negative_review_probabiliy ,bow_not_recom_set, bow_not_recom_dict, df_not_recom]\n",
    "\n",
    "    def classifier(element, your_class = 'positive'):\n",
    "        flag = {}\n",
    "        flag_count = 0\n",
    "        if your_class == 'positive':\n",
    "            df_choices = df_choices_positive\n",
    "        else:\n",
    "            df_choices = df_choices_negative\n",
    "        prob_of_class = df_choices[0]/full_shape\n",
    "        score = 1 * prob_of_class\n",
    "        score = float(format(score, '.12f'))\n",
    "        # score = 0.1\n",
    "        for i in element.split():\n",
    "            if i not in df_choices[2].keys():\n",
    "                pass\n",
    "            else:\n",
    "                prob_word_given_class = (df_choices[2])[i]\n",
    "                prob_word_given_class = float(format(prob_word_given_class, '.12f'))\n",
    "                # Almost the same value, given our spin on this application. \n",
    "                # Normally, this term frequency would be calculated differently across the positive and negative documents\n",
    "                # but we are only looking at the reviews as the documents themselves to determine a word's relevance in the positive\n",
    "                # or negative corpus. \n",
    "                tf = np.log(prob_word_given_class)\n",
    "                # tf = float(format(tf, '.12f'))\n",
    "                # tf = abs(np.log(prob_word_given_class))\n",
    "                # The IDF is the number of reviews / the number of reviews that contain the word in that given corpus\n",
    "                # idf = abs(np.log(df_choices[3].shape[0]/(df_choices[1])[i]))\n",
    "                idf = np.log(df_choices[3].shape[0]/(df_choices[1])[i])\n",
    "                # idf = float(format(idf, '.12f'))\n",
    "                # score *= prob_word_given_class*tf*idf\n",
    "                # score *= prob_word_given_class*idf\n",
    "                score *= prob_word_given_class*tf\n",
    "                # score *= prob_word_given_class\n",
    "\n",
    "                if score < 0:\n",
    "                    flag[flag_count] = (score, your_class)\n",
    "                    print(flag_count,i)\n",
    "                    flag_count += 1\n",
    "\n",
    "\n",
    "        return score\n",
    "    positive_score = classifier(element, 'positive')\n",
    "    negative_score = classifier(element, 'negative')\n",
    "    if positive_score > negative_score:\n",
    "        return True\n",
    "    elif positive_score == negative_score:\n",
    "        # Choosing an arbitrary value, because we assume that a review with one or few words of little substance\n",
    "        # is implied to be negative, as is usual with netizens. \n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sentiment_freq(element,test1=False, test2=False):\n",
    "\n",
    "    \"\"\"This function takes a review and returns the label for that review\"\"\"\n",
    "\n",
    "    full_shape = df.shape[0]\n",
    "    positive_review_probabiliy = len(df_recom)/full_shape\n",
    "    negative_review_probabiliy = len(df_not_recom)/full_shape\n",
    "    \n",
    "    df_choices_positive = [positive_review_probabiliy, bow_recom_set, bow_recom_dict, df_recom]\n",
    "    df_choices_negative = [negative_review_probabiliy ,bow_not_recom_set, bow_not_recom_dict, df_not_recom]\n",
    "\n",
    "    def classifier(element, your_class = 'positive'):\n",
    "        flag = {}\n",
    "        flag_count = 0\n",
    "        if your_class == 'positive':\n",
    "            df_choices = df_choices_positive\n",
    "        else:\n",
    "            df_choices = df_choices_negative\n",
    "        prob_of_class = df_choices[0]/full_shape\n",
    "        score = 1 * prob_of_class\n",
    "        score = float(format(score, '.12f'))\n",
    "        # score = 0.1\n",
    "        for i in element.split():\n",
    "            if i not in df_choices[2].keys():\n",
    "                pass\n",
    "            else:\n",
    "                prob_word_given_class = (df_choices[2])[i]\n",
    "                prob_word_given_class = float(format(prob_word_given_class, '.12f'))\n",
    "                # Almost the same value, given our spin on this application. \n",
    "                # Normally, this term frequency would be calculated differently across the positive and negative documents\n",
    "                # but we are only looking at the reviews as the documents themselves to determine a word's relevance in the positive\n",
    "                # or negative corpus. \n",
    "                tf = np.log(prob_word_given_class)\n",
    "                # tf = float(format(tf, '.12f'))\n",
    "                # tf = abs(np.log(prob_word_given_class))\n",
    "                # The IDF is the number of reviews / the number of reviews that contain the word in that given corpus\n",
    "                # idf = abs(np.log(df_choices[3].shape[0]/(df_choices[1])[i]))\n",
    "                idf = np.log(df_choices[3].shape[0]/(df_choices[1])[i])\n",
    "                # idf = float(format(idf, '.12f'))\n",
    "                # score *= prob_word_given_class*tf*idf\n",
    "                # score *= prob_word_given_class*idf\n",
    "                # score *= prob_word_given_class*tf\n",
    "                score *= prob_word_given_class\n",
    "\n",
    "                if score < 0:\n",
    "                    flag[flag_count] = (score, your_class)\n",
    "                    print(flag_count,i)\n",
    "                    flag_count += 1\n",
    "\n",
    "\n",
    "        return score\n",
    "    positive_score = classifier(element, 'positive')\n",
    "    negative_score = classifier(element, 'negative')\n",
    "    if positive_score > negative_score:\n",
    "        return True\n",
    "    elif positive_score == negative_score:\n",
    "        # Choosing an arbitrary value, because we assume that a review with one or few words of little substance\n",
    "        # is implied to be negative, as is usual with netizens. \n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"score_all\"] = test.Review.apply(define_sentiment_all)\n",
    "test[\"score_tf\"] = test.Review.apply(define_sentiment_tf)\n",
    "test[\"score_idf\"] = test.Review.apply(define_sentiment_idf)\n",
    "test[\"score_freq\"] = test.Review.apply(define_sentiment_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our accuracy for synthetic data when we use all the formula: 45.88%\n",
      "This is our accuracy for synthetic data when we use only frequency and TF: 46.72%\n",
      "This is our accuracy for synthetic data when we use only frequency and IDF: 60.51%\n",
      "This is our accuracy for synthetic data when we use only frequency: 55.41%\n"
     ]
    }
   ],
   "source": [
    "print(f\"This is our accuracy for synthetic data when we use all the formula: {(sum(test['Recommended or Not Recommended'] == test['score_all'])/test.shape[0])*100:.2f}%\")\n",
    "print(f\"This is our accuracy for synthetic data when we use only frequency and TF: {(sum(test['Recommended or Not Recommended'] == test['score_tf'])/test.shape[0])*100:.2f}%\")\n",
    "print(f\"This is our accuracy for synthetic data when we use only frequency and IDF: {(sum(test['Recommended or Not Recommended'] == test['score_idf'])/test.shape[0])*100:.2f}%\")\n",
    "print(f\"This is our accuracy for synthetic data when we use only frequency: {(sum(test['Recommended or Not Recommended'] == test['score_freq'])/test.shape[0])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.33      0.59      0.42      1803\n",
      "        True       0.66      0.39      0.49      3629\n",
      "\n",
      "    accuracy                           0.46      5432\n",
      "   macro avg       0.49      0.49      0.46      5432\n",
      "weighted avg       0.55      0.46      0.47      5432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test['Recommended or Not Recommended'], test['score_all'])\n",
    "# Generate a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test['Recommended or Not Recommended'], test['score_all']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.34      0.64      0.44      1803\n",
      "        True       0.68      0.38      0.49      3629\n",
      "\n",
      "    accuracy                           0.47      5432\n",
      "   macro avg       0.51      0.51      0.47      5432\n",
      "weighted avg       0.57      0.47      0.47      5432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test['Recommended or Not Recommended'], test['score_tf'])\n",
    "# Generate a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test['Recommended or Not Recommended'], test['score_tf']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.40      0.37      0.38      1803\n",
      "        True       0.70      0.72      0.71      3629\n",
      "\n",
      "    accuracy                           0.61      5432\n",
      "   macro avg       0.55      0.55      0.55      5432\n",
      "weighted avg       0.60      0.61      0.60      5432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test['Recommended or Not Recommended'], test['score_idf'])\n",
    "# Generate a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test['Recommended or Not Recommended'], test['score_idf']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.38      0.52      0.44      1803\n",
      "        True       0.70      0.57      0.63      3629\n",
      "\n",
      "    accuracy                           0.55      5432\n",
      "   macro avg       0.54      0.54      0.53      5432\n",
      "weighted avg       0.60      0.55      0.57      5432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test['Recommended or Not Recommended'], test['score_freq'])\n",
    "# Generate a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test['Recommended or Not Recommended'], test['score_freq']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
